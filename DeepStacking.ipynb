{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0977affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ AMD GPU Detected! Using DirectML (dml).\n",
      "üöÄ Current Device: privateuseone:0\n",
      "\n",
      "[Stage 1] Extracting RoBERTa Features...\n",
      " - Fold 1/5 Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:00<00:00, 1013.84it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: klue/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Fold 2/5 Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:00<00:00, 919.26it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: klue/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Fold 3/5 Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:00<00:00, 921.50it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: klue/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Fold 4/5 Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:00<00:00, 966.71it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: klue/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Fold 5/5 Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:00<00:00, 1011.57it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]             \n",
      "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: klue/roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RoBERTa features added to dataset.\n",
      "\n",
      "[Stage 2] Training Main Models with RoBERTa Feature...\n",
      " - Training CatBoost (CPU)...\n",
      " - Training XGBoost (CPU)...\n",
      "\n",
      "[Final Ensemble]\n",
      "‚úÖ Best Weight -> CatBoost: 0.90, XGBoost: 0.10\n",
      "‚úÖ Best Threshold: 0.490\n",
      "‚úÖ Expected F1 Score (OOF): 0.4694\n",
      "\n",
      "Done! Saved to 'submission_stacking_roberta_feat.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Î®∏Ïã†Îü¨Îãù ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# [AMD GPU ÏÑ§Ï†ï]\n",
    "try:\n",
    "    import torch_directml\n",
    "    has_directml = True\n",
    "except ImportError:\n",
    "    has_directml = False\n",
    "    print(\"‚ö†Ô∏è torch-directml ÎØ∏ÏÑ§Ïπò. CPUÎ°ú ÏßÑÌñâÎê©ÎãàÎã§.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 0. ÌôòÍ≤Ω ÏÑ§Ï†ï\n",
    "# ---------------------------\n",
    "SEED = 42\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# [ÌïµÏã¨ Î≥ÄÍ≤Ω] Device ÏÑ§Ï†ï Î°úÏßÅ\n",
    "def get_device():\n",
    "    # 1. DirectML (AMD GPU Ïö∞ÏÑ†)\n",
    "    if has_directml:\n",
    "        print(\"üöÄ AMD GPU Detected! Using DirectML (dml).\")\n",
    "        return torch_directml.device()\n",
    "    \n",
    "    # 2. CUDA (NVIDIA ÌòπÏãúÎÇò Ìï¥ÏÑú)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"üöÄ NVIDIA GPU Detected! Using CUDA.\")\n",
    "        return torch.device(\"cuda\")\n",
    "    \n",
    "    # 3. Apple Silicon (M1/M2/M3)\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    \n",
    "    print(\"üê¢ No GPU detected. Using CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"üöÄ Current Device: {DEVICE}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "# ---------------------------\n",
    "def locate_data_dir():\n",
    "    cwd = Path(\".\").resolve()\n",
    "    if (cwd / \"train.csv\").exists(): return cwd\n",
    "    return cwd \n",
    "\n",
    "DATA_DIR = locate_data_dir()\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "sub = pd.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
    "\n",
    "TARGET = \"completed\"\n",
    "ID_COL = \"ID\"\n",
    "\n",
    "# ---------------------------\n",
    "# 2. ÌÖçÏä§Ìä∏ Ï†ÑÏ≤òÎ¶¨ (RoBERTaÏö©)\n",
    "# ---------------------------\n",
    "MISSING_MARKERS = {\"\", \" \", \"nan\", \"none\", \"null\", \".\", \"-\"}\n",
    "NONE_EQUIV = {\"ÏóÜÏùå\", \"ÏóÜÏäµÎãàÎã§\", \"Ìï¥ÎãπÏóÜÏùå\", \"Î¨¥ÏùëÎãµ\", \"ÎØ∏ÏùëÎãµ\"}\n",
    "\n",
    "def _nfkc(s): return unicodedata.normalize(\"NFKC\", str(s))\n",
    "\n",
    "def normalize_text(x):\n",
    "    if pd.isna(x): return \"\"\n",
    "    s = _nfkc(x).strip()\n",
    "    if s.lower() in MISSING_MARKERS: return \"\"\n",
    "    if s in NONE_EQUIV: return \"ÏóÜÏùå\"\n",
    "    return s\n",
    "\n",
    "feature_cols = [c for c in train.columns if c not in [ID_COL, TARGET]]\n",
    "\n",
    "def make_sentence(row):\n",
    "    segs = []\n",
    "    for c in feature_cols:\n",
    "        val = normalize_text(row[c])\n",
    "        if val: segs.append(f\"{c}:{val}\")\n",
    "    return \" \".join(segs)\n",
    "\n",
    "train['full_text'] = train.apply(make_sentence, axis=1)\n",
    "test['full_text'] = test.apply(make_sentence, axis=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. [Stage 1] RoBERTa Feature Extraction\n",
    "# ---------------------------\n",
    "# RoBERTa Î™®Îç∏ÏùÑ ÌïôÏäµÏãúÏºúÏÑú 'OOF ÏòàÏ∏°Í∞í'Í≥º 'Test ÏòàÏ∏°Í∞í'ÏùÑ ÎΩëÏïÑÎÉÖÎãàÎã§.\n",
    "# Ïù¥ Í∞íÎì§ÏùÄ Îã§Ïùå Îã®Í≥Ñ Î™®Îç∏Ïùò ÌîºÏ≤òÎ°ú ÏÇ¨Ïö©Îê©ÎãàÎã§.\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def extract_roberta_features(train_df, test_df, n_splits=5):\n",
    "    print(\"\\n[Stage 1] Extracting RoBERTa Features...\")\n",
    "    \n",
    "    # Î™®Îç∏ ÏÑ§Ï†ï (Í∞ÄÎ≥çÍ≥† Îπ†Î•∏ small Î™®Îç∏ ÏÇ¨Ïö©, ÏÑ±Îä• ÏöïÏã¨ÎÇòÎ©¥ baseÎ°ú Î≥ÄÍ≤Ω)\n",
    "    MODEL_NM = \"klue/roberta-base\" \n",
    "    BS = 32 # RX 9070XTÎãàÍπå ÎÑâÎÑâÌïòÍ≤å\n",
    "    LR = 1e-5\n",
    "    EPOCHS = 4 # ÌîºÏ≤ò Ï∂îÏ∂úÏö©Ïù¥Îùº Í≥ºÏ†ÅÌï© Î∞©ÏßÄÎ•º ÏúÑÌï¥ Ï†ÅÍ≤å\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NM)\n",
    "    y = train_df[TARGET].values\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    oof_preds = np.zeros(len(train_df))\n",
    "    test_preds = np.zeros(len(test_df))\n",
    "    \n",
    "    # Test Dataset (Ìïú Î≤àÎßå ÏÉùÏÑ±)\n",
    "    te_ds = TextDataset(test_df['full_text'].values, None, tokenizer)\n",
    "    te_dl = DataLoader(te_ds, batch_size=BS, shuffle=False)\n",
    "    \n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(train_df, y)):\n",
    "        print(f\" - Fold {fold+1}/{n_splits} Processing...\")\n",
    "        \n",
    "        # Data Setup\n",
    "        tr_ds = TextDataset(train_df['full_text'].values[tr_idx], y[tr_idx], tokenizer)\n",
    "        va_ds = TextDataset(train_df['full_text'].values[va_idx], y[va_idx], tokenizer)\n",
    "        \n",
    "        tr_dl = DataLoader(tr_ds, batch_size=BS, shuffle=True)\n",
    "        va_dl = DataLoader(va_ds, batch_size=BS, shuffle=False)\n",
    "        \n",
    "        # Model Setup\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NM, num_labels=2)\n",
    "        model.to(DEVICE)\n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        for ep in range(EPOCHS):\n",
    "            for batch in tqdm(tr_dl, desc=f\"Ep {ep+1}\", leave=False):\n",
    "                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "                optim.zero_grad()\n",
    "                out = model(**batch)\n",
    "                loss = out.loss\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "        \n",
    "        # Validation Inference (OOF)\n",
    "        model.eval()\n",
    "        fold_val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in va_dl:\n",
    "                batch = {k: v.to(DEVICE) for k, v in batch.items() if k!='labels'}\n",
    "                out = model(**batch)\n",
    "                prob = torch.softmax(out.logits, dim=1)[:, 1].cpu().numpy()\n",
    "                fold_val_preds.extend(prob)\n",
    "        oof_preds[va_idx] = fold_val_preds\n",
    "        \n",
    "        # Test Inference\n",
    "        fold_test_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in te_dl:\n",
    "                batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "                out = model(**batch)\n",
    "                prob = torch.softmax(out.logits, dim=1)[:, 1].cpu().numpy()\n",
    "                fold_test_preds.extend(prob)\n",
    "        test_preds += np.array(fold_test_preds) / n_splits\n",
    "        \n",
    "        # Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "        del model, optim, tr_dl, va_dl\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        \n",
    "    return oof_preds, test_preds\n",
    "\n",
    "# RoBERTa ÌîºÏ≤ò ÏÉùÏÑ± Ïã§Ìñâ\n",
    "roberta_oof, roberta_test = extract_roberta_features(train, test, n_splits=5)\n",
    "\n",
    "# ÏÉùÏÑ±Îêú ÌîºÏ≤òÎ•º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Ï∂îÍ∞Ä\n",
    "train['roberta_prob'] = roberta_oof\n",
    "test['roberta_prob'] = roberta_test\n",
    "\n",
    "print(\"‚úÖ RoBERTa features added to dataset.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. [Stage 2] Main Models (CatBoost + XGBoost)\n",
    "# ---------------------------\n",
    "print(\"\\n[Stage 2] Training Main Models with RoBERTa Feature...\")\n",
    "\n",
    "# Ï†ÑÏ≤òÎ¶¨ (ÏàòÏπòÌòï/Î≤îÏ£ºÌòï Î∂ÑÎ•ò)\n",
    "# roberta_probÎäî ÏàòÏπòÌòï ÌîºÏ≤òÎ°ú Ï∑®Í∏âÎê©ÎãàÎã§.\n",
    "final_features = [c for c in train.columns if c not in [ID_COL, TARGET, 'full_text']]\n",
    "cat_features = [c for c in final_features if train[c].dtype == 'object']\n",
    "num_features = [c for c in final_features if train[c].dtype != 'object']\n",
    "\n",
    "# Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨ (Í∞ÑÎã®ÌïòÍ≤å)\n",
    "for c in cat_features:\n",
    "    train[c] = train[c].fillna(\"MISSING\").astype(str)\n",
    "    test[c] = test[c].fillna(\"MISSING\").astype(str)\n",
    "for c in num_features:\n",
    "    train[c] = train[c].fillna(0)\n",
    "    test[c] = test[c].fillna(0)\n",
    "\n",
    "# 4-1. CatBoost Training\n",
    "print(\" - Training CatBoost (CPU)...\")\n",
    "cb_oof = np.zeros(len(train))\n",
    "cb_test = np.zeros(len(test))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "y = train[TARGET].values\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(train, y)):\n",
    "    X_tr, X_val = train[final_features].iloc[tr_idx], train[final_features].iloc[va_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[va_idx]\n",
    "    \n",
    "    model = CatBoostClassifier(\n",
    "        iterations=3000,\n",
    "        learning_rate=0.01,\n",
    "        depth=4,\n",
    "        l2_leaf_reg=5,\n",
    "\n",
    "        eval_metric='F1',\n",
    "        random_seed=SEED,\n",
    "        verbose=0,\n",
    "        early_stopping_rounds=300,\n",
    "        cat_features=cat_features,\n",
    "        auto_class_weights='Balanced',\n",
    "        task_type=\"CPU\",  # ÏïàÏ†ÑÌïòÍ≤å CPU ÏÇ¨Ïö©\n",
    "        thread_count=-1\n",
    "    )\n",
    "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), use_best_model=True)\n",
    "    \n",
    "    cb_oof[va_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    cb_test += model.predict_proba(test[final_features])[:, 1] / 5\n",
    "\n",
    "# 4-2. XGBoost Training\n",
    "print(\" - Training XGBoost (CPU)...\")\n",
    "# XGBoostÎäî Î≤îÏ£ºÌòï Îç∞Ïù¥ÌÑ∞Î•º ÏúÑÌï¥ Ïù∏ÏΩîÎî© ÌïÑÏöî (CatBoostÏôÄ Îã¨Î¶¨)\n",
    "# Í∞ÑÎã®ÌïòÍ≤å Ordinal Encoding Ï†ÅÏö©\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "train_xgb = train[final_features].copy()\n",
    "test_xgb = test[final_features].copy()\n",
    "\n",
    "train_xgb[cat_features] = oe.fit_transform(train_xgb[cat_features])\n",
    "test_xgb[cat_features] = oe.transform(test_xgb[cat_features])\n",
    "\n",
    "xgb_oof = np.zeros(len(train))\n",
    "xgb_test = np.zeros(len(test))\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(train, y)):\n",
    "    X_tr, X_val = train_xgb.iloc[tr_idx], train_xgb.iloc[va_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[va_idx]\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=3000,\n",
    "        learning_rate=0.015,\n",
    "        max_depth=4,\n",
    "\n",
    "        # [Ï∂îÍ∞Ä] Í≥ºÏ†ÅÌï© Î∞©ÏßÄ Î∞è ÏùºÎ∞òÌôî ÏÑ±Îä• Ìñ•ÏÉÅ ÏòµÏÖò\n",
    "        min_child_weight=2,       # Í¥ÄÏ∏°Ïπò ÏµúÏÜå Î¨¥Í≤å Ìï© (ÎÖ∏Ïù¥Ï¶àÏóê Í∞ïÌï¥Ïßê)\n",
    "        colsample_bytree=0.8,     # Ìä∏Î¶¨ ÏÉùÏÑ± Ïãú ÌîºÏ≤òÏùò 80%Îßå Î¨¥ÏûëÏúÑ ÏÇ¨Ïö© (Îã§ÏñëÏÑ± ÌôïÎ≥¥)\n",
    "        subsample=0.8,            # Îç∞Ïù¥ÌÑ∞Ïùò 80%Îßå ÏÉòÌîåÎßÅÌï¥ÏÑú ÌïôÏäµ (Î∞∞ÍπÖ Ìö®Í≥º)\n",
    "\n",
    "        eval_metric='logloss',\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=300,\n",
    "        # scale_pos_weightÎäî Î∂àÍ∑†Ìòï Îç∞Ïù¥ÌÑ∞Ïóê Ïú†Ïö© (pos_weight = count(neg) / count(pos))\n",
    "        scale_pos_weight=(len(y_tr) - sum(y_tr)) / sum(y_tr)\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    xgb_oof[va_idx] = model.predict_proba(X_val)[:, 1]\n",
    "    xgb_test += model.predict_proba(test_xgb)[:, 1] / 5\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Ensemble & Threshold Optimization\n",
    "# ---------------------------\n",
    "print(\"\\n[Final Ensemble]\")\n",
    "\n",
    "def get_best_threshold(y_true, y_prob):\n",
    "    best_f1, best_thr = 0, 0.5\n",
    "    for thr in np.arange(0.1, 0.9, 0.01):\n",
    "        pred = (y_prob >= thr).astype(int)\n",
    "        score = f1_score(y_true, pred)\n",
    "        if score > best_f1:\n",
    "            best_f1 = score\n",
    "            best_thr = thr\n",
    "    return best_f1, best_thr\n",
    "\n",
    "# CatBoostÏôÄ XGBoostÎ•º 5:5 ÎòêÎäî ÏµúÏ†Å ÎπÑÏú®Î°ú ÏÑûÍ∏∞\n",
    "best_score = 0\n",
    "best_w = 0.5\n",
    "best_thr = 0.5\n",
    "\n",
    "for w in np.arange(0.0, 1.01, 0.05):\n",
    "    blended_oof = (cb_oof * w) + (xgb_oof * (1 - w))\n",
    "    f1, thr = get_best_threshold(y, blended_oof)\n",
    "    if f1 > best_score:\n",
    "        best_score = f1\n",
    "        best_w = w\n",
    "        best_thr = thr\n",
    "\n",
    "print(f\"‚úÖ Best Weight -> CatBoost: {best_w:.2f}, XGBoost: {1-best_w:.2f}\")\n",
    "print(f\"‚úÖ Best Threshold: {best_thr:.3f}\")\n",
    "print(f\"‚úÖ Expected F1 Score (OOF): {best_score:.4f}\")\n",
    "\n",
    "# Final Prediction\n",
    "final_test_prob = (cb_test * best_w) + (xgb_test * (1 - best_w))\n",
    "final_pred = (final_test_prob >= best_thr).astype(int)\n",
    "\n",
    "# Cap Logic (Í≥ºÎèÑÌïú 1 ÏòàÏ∏° Î∞©ÏßÄ)\n",
    "POS_CAP = 0.70\n",
    "if final_pred.mean() > POS_CAP:\n",
    "    print(f\"‚ö†Ô∏è Applying Positive Cap ({POS_CAP})...\")\n",
    "    n_pos = int(len(final_pred) * POS_CAP)\n",
    "    top_indices = np.argsort(-final_test_prob)[:n_pos]\n",
    "    final_pred[:] = 0\n",
    "    final_pred[top_indices] = 1\n",
    "\n",
    "# Ï†úÏ∂ú ÌååÏùº Ï†ÄÏû•\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission[TARGET] = final_pred\n",
    "submission.to_csv(\"submission_stacking_roberta_feat.csv\", index=False)\n",
    "print(\"\\nDone! Saved to 'submission_stacking_roberta_feat.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amd_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
